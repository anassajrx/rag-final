# Retrieval-Augmented Generation (RAG) System

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Demo](#demo)
- [Installation](#installation)
- [Usage](#usage)
- [How It Works](#how-it-works)
- [Configuration](#configuration)
- [Dependencies](#dependencies)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)
- [Acknowledgements](#acknowledgements)

## Introduction

Welcome to the **Retrieval-Augmented Generation (RAG) System**! This project integrates cutting-edge NLP tools to create a system that processes and retrieves information from PDF documents and generates accurate, context-aware answers to user questions.

The core system combines LangChain for managing language model workflows, ChromaDB for efficient vector storage and retrieval, HuggingFace Embeddings for semantic understanding, and Google's Gemini language model for generating human-like responses. While Streamlit is utilized to create an intuitive front-end interface, the main engine behind the application is built using these powerful technologies.

## Features

- **Multi-PDF Processing**: Upload and process multiple PDF documents to form a robust knowledge base.
- **Contextual Information Retrieval**: Retrieve relevant document sections based on user queries using ChromaDB.
- **Advanced Embedding Generation**: Utilize HuggingFace's `all-MiniLM-L6-v2` model to create meaningful embeddings of document content.
- **Comprehensive Answer Generation**: Generate detailed, user-friendly answers with Google's Gemini model.
- **Streamlit Front-End**: A simple and interactive interface for easy interaction with the RAG system.

## Demo
![WhatsApp Image 2024-08-26 at 17 38 57_f6757b70](https://github.com/user-attachments/assets/b78a3883-cb29-43c7-b568-64ec20552bfc)
![jpg](https://github.com/user-attachments/assets/dbb33383-eeff-4c43-b374-10abdd3e26f7)


## Installation

### Prerequisites

- **Python 3.2 or higher**: Ensure Python is installed.
- **pip**: Python package manager.

### Steps

1. **Clone the Repository**

   ```bash
   git clone https://github.com/yourusername/rag-system.git
   cd rag-system
   ```

2. **Create a Virtual Environment (Optional but Recommended)**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install Required Packages**

   ```bash
   pip install -r requirements.txt
   ```

   *Alternatively, install the necessary packages manually:*

   ```bash
   pip install langchain_community langchain huggingface_hub transformers chromadb google-generativeai streamlit
   ```

4. **Set Up Environment Variables**

   The application requires a Google API key for accessing the Gemini model. Set this as an environment variable:

   ```bash
   export GOOGLE_API_KEY="your-google-api-key"
   ```

   Alternatively, you can configure the API key directly in the code (not recommended).

## Usage

1. **Run the RAG System**

   Navigate to the project directory and run:

   ```bash
   streamlit run app.py
   ```

2. **Interact with the System**

   - **Upload PDFs**: Upload multiple PDF files using the front-end.
   - **Create Vector Store**: Click the "Start Vectorizing" button to process PDFs and create the vector database.
   - **Ask Questions**: Input questions related to the content of the uploaded PDFs and receive answers generated by the RAG system.

     

## How It Works

1. **Document Loading and Splitting**:
   - PDFs are uploaded via the Streamlit interface and processed using `PyPDFLoader`.
   - The text is split into manageable chunks with `RecursiveCharacterTextSplitter` for effective embedding.

2. **Embedding Generation**:
   - Text chunks are embedded using HuggingFace's `all-MiniLM-L6-v2` model, and these embeddings are stored in ChromaDB.

3. **Context Retrieval**:
   - Upon receiving a user query, the system retrieves relevant document sections from the vector database.

4. **Answer Generation**:
   - The relevant context and user query are formatted into a prompt for Google's Gemini model, which generates the final answer.

## Configuration

- **Persist Directory**: Modify the `persist_directory` parameter in the `create_vectorstore` function to change the location where the vector store is saved.
- **Embedding Model**: Adjust the `model_name` in `HuggingFaceEmbeddings` to use a different HuggingFace model if desired.
- **Language Model**: Ensure the correct API key for Google Gemini is configured.

## Dependencies

The core system depends on the following libraries:

- **LangChain**: For managing workflows between language models and vector databases.
- **ChromaDB**: For efficient vector storage and retrieval.
- **HuggingFace Transformers**: For generating embeddings from text.
- **Google Generative AI (genai)**: For generating conversational answers using Gemini.
- **Streamlit**: For the front-end interface.

## Troubleshooting

- **API Key Issues**: Verify that your Google API key is correctly set and has the necessary permissions.
- **Large Files**: Processing large PDFs may require more time and system resources.
- **Dependencies Conflicts**: Create a new virtual environment if you encounter dependency issues.

## Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the Repository**

2. **Create a Feature Branch**

   ```bash
   git checkout -b feature/YourFeature
   ```

3. **Commit Your Changes**

   ```bash
   git commit -m "Add your message"
   ```

4. **Push to the Branch**

   ```bash
   git push origin feature/YourFeature
   ```

5. **Open a Pull Request**

Submit a pull request for review.

## License

*Specify the license under which the project is distributed.*

## Acknowledgements

- **LangChain**: For facilitating integration with language models and vector stores.
- **ChromaDB**: For providing a powerful vector database solution.
- **HuggingFace**: For the pre-trained models used in embedding generation.
- **Google Gemini**: For the language model powering the intelligent responses.
- **Streamlit**: For enabling the creation of an intuitive front-end interface.
